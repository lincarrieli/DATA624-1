---
title: "Predicting Beverage pH"
author: "S. Kigamba, L. Li, P. Maloney, D. Moscoe, and D. Moste"
date: "7/17/2021"
output: word_document
---


## Introduction

pH is a key performance indicator for the beverage manufacturing process. Because beverage products must maintain a pH within a critical range, it's important to understand how pH relates to other quantifiable aspects of beverage manufacturing. In this report, we seek a model for predicting beverage pH based on data about the beverage itself, along with its manufacturing and bottling process.  

In this report, we select the optimal model, and summarize the steps in the model building process. Our criterion for a successful model is low mean absolute percent error (MAPE) when the model is run on test data. In the sections below, we describe the data, sketch our modeling process, and detail the optimal model for predicting pH. We also describe other models that performed nearly as well as the optimal model. 

## About the data

The data set contains information on 2,571 samples of 24-ounce bottled beverages. Most samples comprise information on 33 variables, such as density, temperature, and pH. Overall, less than 1% of values are missing from the data set. We found no pattern in the missing data.  

With the exception of `Brand Code`, every variable is quantitative. Some variables, especially `Hyd Pressure1`, exhibit low variance. Other variables are highly correlated, which suggests the data set contains some redundant information. We also notice significant skewness in some of the variables. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(corrplot)
library(knitr)
```


```{r}
# Load the datasets
initial_import.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")
to_predict.df <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")
```

A correlation plot shows the pairwise correlations in the data set:

```{r}
corr_matrix <- initial_import.df %>%
  keep(is.numeric) %>%
  drop_na() %>%
  cor(method = "pearson")

corrplot::corrplot(corr_matrix, method = "circle", is.corr = TRUE)
```

The response variable, pH, is roughly normally distributed, with mean 8.55 and standard deviation 0.173.  

```{r}
ggplot(data = initial_import.df, aes(x = PH)) +
  geom_histogram() +
  xlab("pH") +
  ylab("Frequency") +
  ggtitle("Response Variable pH is Roughly Normally Distributed")
```

The explanatory variables exhibit a variety of distributions.

```{r}
initial_import.df %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") + 
  geom_histogram()
```

## Our modeling process

In this report we explore a range of linear models, tree-based models, and neural networks to identify a procedure that is highly accurate in predicting the pH of a previously unseen beverage. For each model, we take the following steps:  

(1) Impute missing data if necessary;
(2) Transform data to address skewness, outliers, and low-variance variables;
(3) Check that data conform to the assumptions of the model;
(4) Fit a model and use cross-validation or another procedure to optimize parameters;
(5) Examine residuals;
(6) Compute model metrics.


## Summary of models

We built six models in total and used MAPE and RMSE scores to evaluate model performance. The summary table below shows the models and their corresponding MAPE and RMSE scores. We noticed that distance and regression tree models performed the best, followed by linear models. Neural Networks (nonlinear) model had the worst performance overall.


```{r results = 'asis'}

Type <- c("OLS", "PLS", "Elastic Net", "KNN", "Neural Nets", "Random Forest")
Parameters <- c("None", "Components = 13", "Lambda = 0, Fraction = 1", "y", "Hidden Units", "ntrees")
MAPE <- c(1.22, 1.19, 1.24, 0.91, 1.34, 0.93)
RMSE <- c(0.135, 0.134, 0.139, 0.11, 0.14, 0.10)

df <- data.frame(Type, Parameters, MAPE, RMSE)
df_sort <- df[with(df, order(-MAPE)), ]
#Put table in descending order"MAPE"APE
kable(df_sort, caption = "Summary of models")

```

## Optimal model: Random Foerst
Our best two performing models are K-Nearest Neighbors and Random Forests, both with a MAPE of < 1.0. In this report we will focus on the Random Forest model. Random Forest model is an ensemble tree-based learning algorithm that averages the prediction over many individual trees. The algorithm uses bootstrap aggregation, or bagging to reduce over fitting and improve accuracy. 

Random Forest models are easy to interpret, and are flexible with both regression and classification problems. They work well with categorical and continuous variables, and can handle large datasets without the data normalization requirement. 


- Go carefully through each step in "Our modeling process."
- Any guesses about why this was the minimum-MAPE model?
- Can you think of any next steps you might take to improve the model even further?


```{r}
data <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentDataTOMODEL.csv")

library(plyr)
data[,1] <- mapvalues(data[,1],
                      from = c("A","B","C","D",""),
                      to = c(1,2,3,4,NA))
data[,1] <- as.integer(data[,1])

# Removing the response variable since I don't want to impute or transform these values
drops <- c("PH")
features <- data[,!(names(data) %in% drops)]

na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
features[] <- lapply(features, na_to_mean)

processed <- cbind(data[,26], features)
names(processed)[1] <- ("PH")

# Checking if any of the pH data is missing
summary(processed$PH)

processed <- processed[complete.cases(processed),]
```

# Split Data and Train Model

```{r}
set.seed(12345)
train_ind <- sample(seq_len(nrow(processed)),
                    size = floor(0.75*nrow(processed)))

train <- processed[train_ind,]
test <- processed[-train_ind,]
```

lets run a simple random forest model as a baseline

```{r}
library(ggplot2)
library(tidyverse)
library(caret)
library(randomForest)

rf <- randomForest(PH ~ ., data = train, ntrees = 500)
varImpPlot(rf)
rf
test_rf <- predict(rf, test)

caret_test_rf <- data.frame(cbind(test_rf,test[,1]))
colnames(caret_test_rf) <- c("caret","actual")
caret_test_rf <- caret_test_rf %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE <- (mean(caret_test_rf$pe))*100
MAPE

ggplot(caret_test_rf, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()

```

```{r}
library(gbm)

boosted <- gbm(PH ~ ., data = train, distribution = "gaussian", n.trees = 500, shrinkage = 0.1)
boosted
test_boosted <- predict(boosted, test)

caret_test_boosted <- data.frame(cbind(test_boosted,test[,1]))
colnames(caret_test_boosted) <- c("caret","actual")
caret_test_boosted <- caret_test_boosted %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE <- (mean(caret_test_boosted$pe))*100
MAPE

ggplot(caret_test_boosted, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()
```

```{r}

features2 <- data[,!(names(data) %in% drops)]

na_to_med <- function(x) replace(x, is.na(x), median(x, na.rm = TRUE))
features2[] <- lapply(features2, na_to_med)

trans <- preProcess(features2,
                    method = c("BoxCox", "center", "scale"))
transformed_feat <- predict(trans, features2)

processed2 <- cbind(data[,26], transformed_feat)
names(processed2)[1] <- ("PH")


processed2 <- processed2[complete.cases(processed2),]

#split data
set.seed(12345)
train_ind <- sample(seq_len(nrow(processed)),
                    size = floor(0.75*nrow(processed)))

train2 <- processed[train_ind,]
test2 <- processed[-train_ind,]

#model

rf2 <- randomForest(PH ~ ., data = train2, ntrees = 500)
varImpPlot(rf2)
rf2
test_rf2 <- predict(rf2, test2)

caret_test_rf2 <- data.frame(cbind(test_rf2,test2[,1]))
colnames(caret_test_rf2) <- c("caret","actual")
caret_test_rf2 <- caret_test_rf2 %>%
  mutate(pe = abs(actual - caret)/actual)

MAPE2 <- (mean(caret_test_rf2$pe))*100
MAPE2

ggplot(caret_test_rf2, aes(x = actual, y = caret)) +
  geom_line() +
  geom_point()
```

The model with the transformed predictor variables and median imputation produced the same MAPE value as the baseline model. This makes some sense since the random forest algorithm is based on partitioning of the data by certain variable values. 

Predictions

```{r}
data2 <- read.csv("https://raw.githubusercontent.com/dmoste/DATA624/main/Project2/StudentEvaluationTOPREDICT.csv")

data2[,1] <- mapvalues(data2[,1],
                      from = c("A","B","C","D",""),
                      to = c(1,2,3,4,NA))
data2[,1] <- as.integer(data2[,1])

# Removing the response variable since I don't want to impute or transform these values
drops <- c("PH")
features3 <- data2[,!(names(data2) %in% drops)]

na_to_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
features3[] <- lapply(features3, na_to_mean)


preds <- predict(rf, features3)

#preds

```

## Other models
- link to repo 


## Conclusion

- Was there a clear winner, or did several models perform roughly equally?
- Any data we wish we had, but don't?
{"mode":"full","isActive":false}
